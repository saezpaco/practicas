{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan de Experimento - Evaluación generación de ontologías con y sin RAG\n",
    "\n",
    "**Objetivo:** Evaluar la capacidad del LLM (Mistral-7B-Instruct-v0.3) para generar TTL válido:\n",
    "- Sin RAG (zero-shot)\n",
    "- Con RAG (con contexto de la ontología oficial + dataset)\n",
    "\n",
    "**Ontología oficial:** tad.txt  \n",
    "**Dataset:** CancerEnD_1000.nt  \n",
    "**Métricas:**\n",
    "- Cobertura clases %\n",
    "- Cobertura propiedades %\n",
    "- Nuevas propiedades\n",
    "- Inconsistencias\n",
    "- Nota manual (calidad TTL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports y configuración\n",
    "from rdflib import Graph, URIRef\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from transformers import pipeline\n",
    "\n",
    "model_path_embed = \"/Users/franciscosaez/.lmstudio/models/sentence_transformers/all-MiniLM-L6-v2\"\n",
    "model_path_llm = \"/Users/franciscosaez/Downloads/Mistral-7B-Instruct-v0.3\"\n",
    "ttl_path = \"tad.txt\"\n",
    "nt_path = \"./ontologias_experimentos_RAG/CancerEnD_1000.nt\"\n",
    "\n",
    "# Inicializar modelos\n",
    "model = SentenceTransformer(model_path_embed)\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.get_or_create_collection(\"biological_data_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexación TTL / NT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTL\n",
    "g = Graph()\n",
    "g.parse(ttl_path, format=\"turtle\")\n",
    "\n",
    "ttl_fragments = []\n",
    "for s in g.subjects():\n",
    "    triples = list(g.triples((s, None, None)))\n",
    "    if triples:\n",
    "        frag = \"\\n\".join([f\"{str(subj)} {str(pred)} {str(obj)} .\" for (subj, pred, obj) in triples])\n",
    "        ttl_fragments.append(frag)\n",
    "\n",
    "print(f\"TTL: {len(ttl_fragments)} fragmentos.\")\n",
    "\n",
    "# NT\n",
    "with open(nt_path, \"r\") as file:\n",
    "    tripletas = [line.strip() for line in file.readlines() if line.strip()]\n",
    "\n",
    "block_size = 20\n",
    "nt_fragments = [\n",
    "    \"\\n\".join(tripletas[i:i+block_size])\n",
    "    for i in range(0, len(tripletas), block_size)\n",
    "]\n",
    "\n",
    "print(f\"NT: {len(nt_fragments)} fragmentos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserción en ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTL\n",
    "for i, frag in enumerate(ttl_fragments):\n",
    "    embedding = model.encode([frag])[0].tolist()\n",
    "    collection.add(\n",
    "        documents=[frag],\n",
    "        ids=[f\"ttl_block_{i}\"],\n",
    "        embeddings=[embedding]\n",
    "    )\n",
    "\n",
    "# NT\n",
    "for i, frag in enumerate(nt_fragments):\n",
    "    embedding = model.encode([frag])[0].tolist()\n",
    "    collection.add(\n",
    "        documents=[frag],\n",
    "        ids=[f\"nt_block_{i}\"],\n",
    "        embeddings=[embedding]\n",
    "    )\n",
    "\n",
    "print(\"Indexación completa ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query y recuperación de fragmentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"gene-enhancer associations with anatomical context\"\n",
    "query_embedding = model.encode([query])[0].tolist()\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "for doc_id, doc_content in zip(results['ids'][0], results['documents'][0]):\n",
    "    print(f\"\\nID: {doc_id}\\n---\\n{doc_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación Prompt automática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_fragment = None\n",
    "nt_fragment = None\n",
    "\n",
    "for doc_id, doc_content in zip(results['ids'][0], results['documents'][0]):\n",
    "    if ttl_fragment is None and doc_id.startswith(\"ttl_block_\"):\n",
    "        ttl_fragment = doc_content\n",
    "    if nt_fragment is None and doc_id.startswith(\"nt_block_\"):\n",
    "        nt_fragment = doc_content\n",
    "    if ttl_fragment and nt_fragment:\n",
    "        break\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are an expert in semantic knowledge modeling and ontology engineering.\n",
    "\n",
    "Ontology fragment:\n",
    "{ttl_fragment}\n",
    "\n",
    "Dataset fragment:\n",
    "{nt_fragment}\n",
    "\n",
    "Task:\n",
    "- For each subject in the dataset, propose:\n",
    "    - The correct class (from ontology or new if needed)\n",
    "    - The mapping of each property to an ontology property\n",
    "    - If no property matches, propose a new one with domain/range.\n",
    "\n",
    "Return your output as a valid Turtle RDF block.\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llamada al LLM (Mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_path_llm,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "resultado = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=2048\n",
    ")\n",
    "\n",
    "respuesta_llm = resultado[0][\"generated_text\"]\n",
    "print(respuesta_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar TTL generado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"./ontologias_experimentos_RAG/ontologia_generada_Q1.ttl\"\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(respuesta_llm)\n",
    "\n",
    "print(f\"Ontología generada guardada en: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación automática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_path = \"./ontologias_experimentos_RAG/ontologia_generada_Q1.ttl\"\n",
    "\n",
    "official_graph = Graph()\n",
    "official_graph.parse(ttl_path, format=\"turtle\")\n",
    "\n",
    "generated_graph = Graph()\n",
    "generated_graph.parse(generated_path, format=\"turtle\")\n",
    "\n",
    "official_classes = set(official_graph.subjects(predicate=None, object=URIRef(\"http://www.w3.org/2002/07/owl#Class\")))\n",
    "generated_classes = set(generated_graph.subjects(predicate=None, object=URIRef(\"http://www.w3.org/2002/07/owl#Class\")))\n",
    "\n",
    "official_properties = set(official_graph.predicates())\n",
    "generated_properties = set(generated_graph.predicates())\n",
    "\n",
    "coverage_classes = len(generated_classes & official_classes) / len(official_classes) * 100\n",
    "coverage_properties = len(generated_properties & official_properties) / len(official_properties) * 100\n",
    "\n",
    "new_properties = generated_properties - official_properties\n",
    "missing_properties = official_properties - generated_properties\n",
    "\n",
    "print(f\"Coverage classes: {coverage_classes:.1f}%\")\n",
    "print(f\"Coverage properties: {coverage_properties:.1f}%\")\n",
    "print(f\"Nuevas propiedades: {len(new_properties)}\")\n",
    "print(f\"Propiedades faltantes: {len(missing_properties)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
